{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525bf509",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "     \n",
    "       Prints the result to stdout and returns the exit status. \n",
    "       Provides a printed warning on non-zero exit status unless `warn` \n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs236299-2022-spring/project2.git .tmp\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9baa12c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9918757a",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe802350",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d00522",
   "metadata": {
    "colab_type": "text",
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# 236299 - Introduction to Natural Language Processing\n",
    "## Project 2: Sequence labeling â€“ The slot filling task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef3057",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The second segment of the project involves a sequence labeling task, in which the goal is to label the tokens in a text. Many NLP tasks have this general form. Most famously is the task of _part-of-speech labeling_ as you explored in lab 2-4, where the tokens in a text are to be labeled with their part of speech (noun, verb, preposition, etc.). In this project segment, however, you'll use sequence labeling to implement a system for filling the slots in a template that is intended to describe the meaning of an ATIS query. For instance, the sentence \n",
    "\n",
    "    What's the earliest arriving flight between Boston and Washington DC?\n",
    "    \n",
    "might be associated with the following slot-filled template: \n",
    "\n",
    "    flight_id\n",
    "        fromloc.cityname: boston\n",
    "        toloc.cityname: washington\n",
    "        toloc.state: dc\n",
    "        flight_mod: earliest arriving\n",
    "    \n",
    "You may wonder how this task is a sequence labeling task. We label each word in the source sentence with a tag taken from a set of tags that correspond to the slot-labels. For each slot-label, say `flight_mod`, there are two tags: `B-flight_mod` and `I-flight_mod`. These are used to mark the beginning (B) or interior (I) of a phrase that fills the given slot. In addition, there is a tag for other (O) words that are not used to fill any slot. (This technique is thus known as IOB encoding.) Thus the sample sentence would be labeled as follows:\n",
    "\n",
    "| Token   | Label    |\n",
    "| :------ | :----- | \n",
    "| `BOS` | `O` |\n",
    "| `what's` | `O` |\n",
    "| `the` | `O` |\n",
    "| `earliest` | `B-flight_mod` |\n",
    "| `arriving` | `I-flight_mod` |\n",
    "| `flight` | `O` |\n",
    "| `between` | `O` |\n",
    "| `boston` | `B-fromloc.city_name` |\n",
    "| `and` | `O` |\n",
    "| `washington` | `B-toloc.city_name` |\n",
    "| `dc` | `B-toloc.state_code` |\n",
    "| `EOS` | `O` |\n",
    "\n",
    "> See below for information about the `BOS` and `EOS` tokens. \n",
    "\n",
    "The template itself is associated with the question type for the sentence, perhaps as recovered from the sentence in the last project segment.\n",
    "\n",
    "In this segment, you'll implement three methods for sequence labeling: a hidden Markov model (HMM) and two recurrent neural networks, a simple RNN and a long short-term memory network (LSTM). By the end of this homework, you should have grasped the pros and cons of the statistical and neural approaches.\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Implement an HMM-based approach to sequence labeling.\n",
    "2. Implement an RNN-based approach to sequence labeling.\n",
    "3. Implement an LSTM-based approach to sequence labeling.\n",
    "4. Compare the performances of HMM and RNN/LSTM with different amounts of training data. Discuss the pros and cons of the HMM approach and the neural approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf76999",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1b1cd0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import wget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.legacy as tt\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c41bdcb",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# GPU check, sets runtime type to \"GPU\" where available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f4beb",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Loading data\n",
    "\n",
    "We download the ATIS dataset, already presplit into training, validation (dev), and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71cca38",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Prepare to download needed data\n",
    "def download_if_needed(filename, source='./', dest='./'):\n",
    "    os.makedirs(data_path, exist_ok=True) # ensure destination\n",
    "    os.path.exists(f\"./{dest}{filename}\") or wget.download(source + filename, out=dest)\n",
    "\n",
    "source_path = \"https://raw.githubusercontent.com/nlp-course/data/master/ATIS/\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Download files\n",
    "for filename in [\"atis.train.txt\",\n",
    "                 \"atis.dev.txt\",\n",
    "                 \"atis.test.txt\"\n",
    "                ]:\n",
    "    download_if_needed(filename, source_path, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b6a28",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We again use `torchtext` to load data and convert words to indices in the vocabulary. We use one field `TEXT` for processing the question, and another field `TAG` for processing the sequence labels.\n",
    "\n",
    "We treat words occurring fewer than three times in the training data as _unknown words_. They'll be replaced by the unknown word type `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971509fa",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 3\n",
    "\n",
    "TEXT = tt.data.Field(init_token=\"<bos>\", batch_first=False) # batches are of size max_len x bsz\n",
    "TAG = tt.data.Field(init_token=\"<bos>\", batch_first=False)  # ditto\n",
    "fields = (('text', TEXT), ('tag', TAG))\n",
    "\n",
    "train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n",
    "  fields=fields, \n",
    "  path='./data/', \n",
    "  train='atis.train.txt',\n",
    "  validation='atis.dev.txt',\n",
    "  test='atis.test.txt'\n",
    ")\n",
    "\n",
    "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n",
    "TAG.build_vocab(train.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf2db1",
   "metadata": {},
   "source": [
    "We can get some sense of the datasets by looking at the size and some elements of the text and tag vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc4cc65",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English vocabulary: 518\n",
      "Most common English words: [('BOS', 4274), ('EOS', 4274), ('to', 3682), ('from', 3203), ('flights', 2075), ('the', 1745), ('on', 1343), ('flight', 1035), ('me', 1005), ('what', 985)]\n",
      "\n",
      "Number of tags: 104\n",
      "Most common tags: [('O', 38967), ('B-toloc.city_name', 3751), ('B-fromloc.city_name', 3726), ('I-toloc.city_name', 1039), ('B-depart_date.day_name', 835), ('I-fromloc.city_name', 636), ('B-airline_name', 610), ('B-depart_time.period_of_day', 555), ('I-airline_name', 374), ('B-depart_date.day_number', 351)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of English vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Most common English words: {TEXT.vocab.freqs.most_common(10)}\\n\")\n",
    "\n",
    "print(f\"Number of tags: {len(TAG.vocab)}\")\n",
    "print(f\"Most common tags: {TAG.vocab.freqs.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ef343",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Special tokens and tags\n",
    "\n",
    "You'll have already noticed the `BOS` and `EOS`, special tokens that the dataset developers used to indicate the beginning and end of the sentence; we'll leave them in the data.\n",
    "\n",
    "We've also passed in `init_token=\"<bos>\"` for both torchtext fields. Torchtext will prepend these to the sequence of words and tags. This relieves us from estimating the initial distribution of tags and tokens in HMMs, since we always start with a token `<bos>` whose tag is also `<bos>`. We'll be able to refer to these tags as exemplified here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3737a7",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial tag string: <bos>\n",
      "Initial tag id:     2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Initial tag string: {TAG.init_token}\n",
    "Initial tag id:     {TAG.vocab.stoi[TAG.init_token]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38608ec",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, since `torchtext` will be providing the sentences in the training corpus in \"batches\", `torchtext` will force the sentences within a batch to be the same length by padding them with a special token. Again, we can access that token as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cd15a6",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pad tag string: <pad>\n",
      "Pad tag id:     1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Pad tag string: {TAG.pad_token}\n",
    "Pad tag id:     {TAG.vocab.stoi[TAG.pad_token]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03de9a4",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, we can iterate over the dataset using `torchtext`'s iterator. We'll use a non-trivial batch size to gain the benefit of training on multiple sentences at a shot. You'll need to be careful about the shapes of the various tensors that are being manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed69f956",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "train_iter, val_iter, test_iter = tt.data.BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    repeat=False, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314896c",
   "metadata": {},
   "source": [
    "Each batch will be a tensor of size `max_length x batch_size`. Let's examine a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d903dc",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch text tensor: torch.Size([22, 20])\n",
      "\n",
      "First sentence in batch\n",
      "tensor([ 2,  3, 21, 45, 88, 44,  7, 39, 28, 20, 54, 18, 22,  4,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1])\n",
      "<bos> BOS i need information for flights leaving baltimore and arriving in atlanta EOS <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "First tags in batch\n",
      "tensor([2, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "['<bos>', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'O', 'O', 'B-toloc.city_name', 'O', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "# What's its shape? Should be max_length x batch_size.\n",
    "print(f'Shape of batch text tensor: {batch.text.shape}\\n')\n",
    "\n",
    "# Extract the first sentence in the batch, both text and tags\n",
    "first_sentence = batch.text[:, 0]\n",
    "first_tags = batch.tag[:, 0]\n",
    "\n",
    "# Print out the first sentence, as token ids and as text\n",
    "print(\"First sentence in batch\")\n",
    "print(f\"{first_sentence}\")\n",
    "print(f\"{' '.join([TEXT.vocab.itos[i] for i in first_sentence])}\\n\")\n",
    "\n",
    "print(\"First tags in batch\")\n",
    "print(f\"{first_tags}\")\n",
    "print(f\"{[TAG.vocab.itos[i] for i in first_tags]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78014cd8",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The goal of this project is to predict the sequence of tags `batch.tag` given a sequence of words `batch.text`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5892a1a",
   "metadata": {},
   "source": [
    "# Majority class labeling\n",
    "\n",
    "As usual, we can get a sense of the difficulty of the task by looking at a simple baseline, tagging every token with the majority tag. Here's a table of tag frequencies for the most frequent tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "944d5ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  <unk>                           0\n",
      "  1  <pad>                           0\n",
      "  2  <bos>                         4274\n",
      "  3  O                             38967\n",
      "  4  B-toloc.city_name             3751\n",
      "  5  B-fromloc.city_name           3726\n",
      "  6  I-toloc.city_name             1039\n",
      "  7  B-depart_date.day_name        835\n",
      "  8  I-fromloc.city_name           636\n",
      "  9  B-airline_name                610\n",
      " 10  B-depart_time.period_of_day   555\n",
      " 11  I-airline_name                374\n",
      " 12  B-depart_date.day_number      351\n",
      " 13  B-depart_date.month_name      340\n",
      " 14  B-depart_time.time            321\n",
      " 15  B-round_trip                  311\n",
      " 16  I-round_trip                  303\n",
      " 17  B-depart_time.time_relative   290\n",
      " 18  B-cost_relative               281\n",
      " 19  B-flight_mod                  264\n",
      " 20  I-depart_time.time            258\n",
      " 21  B-stoploc.city_name           202\n",
      " 22  B-city_name                   191\n",
      " 23  B-arrive_time.time            182\n",
      " 24  B-class_type                  181\n",
      " 25  B-arrive_time.time_relative   162\n",
      " 26  I-class_type                  148\n",
      " 27  I-arrive_time.time            142\n",
      " 28  B-flight_stop                 141\n",
      " 29  B-airline_code                109\n",
      " 30  I-depart_date.day_number      105\n",
      " 31  I-fromloc.airport_name        103\n",
      " 32  B-toloc.state_name             84\n",
      " 33  B-toloc.state_code             81\n",
      " 34  B-arrive_date.day_name         78\n",
      " 35  B-fromloc.airport_name         75\n",
      " 36  B-depart_date.date_relative    72\n",
      " 37  B-flight_number                72\n",
      " 38  B-depart_date.today_relative   70\n",
      " 39  I-airport_name                 61\n",
      " 40  I-city_name                    53\n",
      " 41  B-arrive_time.period_of_day    51\n",
      " 42  B-fare_basis_code              51\n",
      " 43  B-flight_time                  51\n",
      " 44  B-fromloc.state_code           51\n",
      " 45  B-or                           49\n",
      " 46  B-aircraft_code                48\n",
      " 47  B-meal_description             48\n",
      " 48  B-meal                         47\n",
      " 49  I-cost_relative                45\n",
      " 50  I-stoploc.city_name            45\n",
      " 51  B-airport_name                 44\n",
      " 52  B-transport_type               43\n",
      " 53  B-fromloc.state_name           42\n",
      " 54  B-arrive_date.day_number       40\n",
      " 55  B-arrive_date.month_name       40\n",
      " 56  B-depart_time.period_mod       39\n",
      " 57  B-flight_days                  37\n",
      " 58  B-connect                      36\n",
      " 59  I-toloc.airport_name           35\n",
      " 60  B-fare_amount                  34\n",
      " 61  I-fare_amount                  33\n",
      " 62  B-economy                      32\n",
      " 63  B-toloc.airport_name           28\n",
      " 64  B-mod                          24\n",
      " 65  I-flight_time                  24\n",
      " 66  B-airport_code                 22\n",
      " 67  B-depart_date.year             20\n",
      " 68  B-toloc.airport_code           19\n",
      " 69  B-arrive_time.start_time       18\n",
      " 70  B-depart_time.end_time         18\n",
      " 71  B-depart_time.start_time       18\n",
      " 72  I-transport_type               18\n",
      " 73  B-arrive_time.end_time         17\n",
      " 74  I-arrive_time.end_time         16\n",
      " 75  B-fromloc.airport_code         14\n",
      " 76  B-restriction_code             14\n",
      " 77  I-depart_time.end_time         13\n",
      " 78  I-flight_mod                   12\n",
      " 79  I-flight_stop                  12\n",
      " 80  B-arrive_date.date_relative    10\n",
      " 81  I-toloc.state_name             10\n",
      " 82  I-restriction_code              9\n",
      " 83  B-return_date.date_relative     8\n",
      " 84  I-depart_time.start_time        8\n",
      " 85  I-economy                       8\n",
      " 86  B-state_code                    7\n",
      " 87  I-arrive_time.start_time        7\n",
      " 88  I-fromloc.state_name            7\n",
      " 89  B-state_name                    6\n",
      " 90  I-depart_date.today_relative    6\n",
      " 91  I-depart_time.period_of_day     5\n",
      " 92  B-period_of_day                 4\n",
      " 93  I-arrive_date.day_number        4\n",
      " 94  B-day_name                      3\n",
      " 95  B-meal_code                     3\n",
      " 96  B-stoploc.state_code            3\n",
      " 97  B-arrive_time.period_mod        2\n",
      " 98  B-toloc.country_name            2\n",
      " 99  I-arrive_time.time_relative     2\n",
      "100  I-meal_code                     2\n",
      "101  I-return_date.date_relative     2\n",
      "102  B-return_date.day_number        1\n",
      "103  B-return_date.month_name        1\n"
     ]
    }
   ],
   "source": [
    "def count_tags(iterator):\n",
    "  tag_counts = torch.zeros(len(TAG.vocab.itos), device=device)\n",
    "\n",
    "  for batch in iterator:\n",
    "    tags = batch.tag.view(-1)\n",
    "    tag_counts.scatter_add_(0, tags, torch.ones(tags.shape).to(device))\n",
    "\n",
    "  ## Alternative untensorized implementation for reference\n",
    "  # for batch in iterator:                # for each batch\n",
    "  #   for sent_id in range(len(batch)):   # ... each sentence in the batch\n",
    "  #     for tag in batch.tag[:, sent_id]: # ... each tag in the sentence\n",
    "  #       tag_counts[tag] += 1            # bump the tag count\n",
    "\n",
    "  # Ignore paddings\n",
    "  tag_counts[TAG.vocab.stoi[TAG.pad_token]] = 0\n",
    "  return tag_counts\n",
    "\n",
    "tag_counts = count_tags(train_iter)\n",
    "\n",
    "for tag_id in range(len(TAG.vocab.itos)):\n",
    "  print(f'{tag_id:3}  {TAG.vocab.itos[tag_id]:30}{tag_counts[tag_id].item():3.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758de818",
   "metadata": {},
   "source": [
    "It looks like the `'O'` (other) tag is, unsurprisingly, the most frequent tag (except for the padding tag). The proportion of tokens labeled with that tag (ignoring the padding tag) gives us a good baseline accuracy for this sequence labeling task. To verify that intuition, we can calculate the accuracy of the majority tag on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a32b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.634\n"
     ]
    }
   ],
   "source": [
    "tag_counts_test = count_tags(test_iter)\n",
    "majority_baseline_accuracy = (\n",
    "  tag_counts_test[TAG.vocab.stoi['O']] \n",
    "  / tag_counts_test.sum()\n",
    ")\n",
    "print(f'Baseline accuracy: {majority_baseline_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9302c",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# HMM for sequence labeling\n",
    "\n",
    "Having established the baseline to beat, we turn to implementing an HMM model.\n",
    "\n",
    "## Notation\n",
    "\n",
    "First, let's start with some notation. We use $\\mathcal{V} = \\langle \\mathcal{V}_1, \\mathcal{V}_2, \\ldots \\mathcal{V}_V \\rangle$ to denote the vocabulary of word types and $Q = \\langle{Q_1, Q_2, \\ldots, Q_N} \\rangle$ to denote the possible tags, which is the state space of the HMM. Thus $V$ is the number of word types in the vocabulary and $N$ is the number of states (tags).\n",
    "\n",
    "We use $\\vect{w} = w_1 \\cdots w_T \\in \\mathcal{V}^T$ to denote the string of words at \"time steps\" $t$ (where $t$ varies from $1$ to $T$). Similarly, $\\vect{q} = q_1 \\cdots q_T \\in Q^T$ denotes the corresponding sequence of states (tags)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cbd27",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training an HMM by counting\n",
    "\n",
    "Recall that an HMM is defined via a transition matrix $A$, which stores the probability of moving from one state $Q_i$ to another $Q_j$, that is, \n",
    "\n",
    "$$A_{ij}=\\Prob(q_{t+1}=Q_j  \\given  q_t=Q_i)$$\n",
    "\n",
    "and an emission matrix $B$, which stores the probability of generating word $\\mathcal{V}_j$ given state $Q_i$, that is, \n",
    "\n",
    "$$B_{ij}= \\Prob(w_t=\\mathcal{V}_j  \\given q_t= Q_i)$$\n",
    "\n",
    "> As is typical in notating probabilities, we'll use abbreviations\n",
    ">\n",
    "\\begin{align}\n",
    "\\Prob(q_{t+1} \\given  q_t) &\\equiv \\Prob(q_{t+1}=Q_j  \\given  q_t=Q_i) \\\\\n",
    "\\Prob(w_t  \\given q_t) &\\equiv \\Prob(w_t=\\mathcal{V}_j  \\given q_t= Q_i)\n",
    "\\end{align}\n",
    ">\n",
    "> where the $i$ and $j$ are clear from context.\n",
    "\n",
    "In our case, since the labels are observed in the training data, we can directly use counting to determine (maximum likelihood) estimates of $A$ and $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9520cd",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Goal 1(a): Find the transition matrix\n",
    "\n",
    "The matrix $A$ contains the transition probabilities: $A_{ij}$ is the probability of moving from state $Q_i$ to state $Q_j$ in the training data, so that $\\sum^{N}_{j = 1 } A_{ij} = 1$ for all $i$. \n",
    "\n",
    "We find these probabilities by counting the number of times state $Q_j$ appears right after state $Q_i$, as a proportion of all of the transitions from $Q_i$.\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{\\cnt{Q_i, Q_j} + \\delta}{\\sum_k \\left (\\cnt{Q_i, Q_k}+\\delta \\right)}\n",
    "$$\n",
    "\n",
    "(In the above formula, we also used add-$\\delta$ smoothing.)\n",
    "\n",
    "Using the above definition, implement the method `train_A` in the `HMM` class below, which calculates and returns the $A$ matrix as a tensor of size $N \\times N$.\n",
    "\n",
    "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal.\n",
    "\n",
    "> Remember that the training data is being delivered to you batched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecefdf7",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Goal 1(b): Find the emission matrix $B$\n",
    "\n",
    "Similar to the transition matrix, the emission matrix contains the emission probabilities such that $B_{ij}$ is probability of word $w_t=\\mathcal{V}_j$ conditioned on state $q_t=Q_i$.\n",
    "\n",
    "We can find this by counting as well.\n",
    "$$\n",
    "B_{ij} = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\sum_k \\left (\\cnt{Q_i, \\mathcal{V}_k} + \\delta \\right)}\n",
    "       = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\cnt{Q_i} + \\delta V}\n",
    "$$\n",
    "\n",
    "Using the above definitions, implement the `train_B` method in the `HMM` class below, which calculates and returns the $B$ matrix as a tensor of size $N \\times V$.\n",
    "\n",
    "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f352e",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Sequence labeling with a trained HMM\n",
    "\n",
    "Now that you're able to train an HMM by estimating the transition matrix $A$ and the emission matrix $B$, you can apply it to the task of labeling a sequence of words $\\vect{w} = w_1 \\cdots w_T$. Our goal is to find the most probable sequence of tags $\\vect{\\hat q} \\in Q^T$ given a sequence of words $\\vect{w} \\in \\mathcal{V}^T$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\vect{\\hat q} &= \\operatorname*{argmax}\\limits_{\\vect{q} \\in Q^T}(\\Prob(\\vect{q} \\given \\vect{w})) \\\\\n",
    "& = \\operatorname*{argmax}_{\\vect{q} \\in Q^T}(\\Prob(\\vect{q},\\vect{w})) \\\\\n",
    "& = \\operatorname*{argmax}_{\\vect{q} \\in Q^T}\\left(\\Pi^{T}_{t = 1} \\Prob(w_{t} \\given q_{t})\\Prob(q_{t} \\given q_{t-1})\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\Prob(w_{t}=\\mathcal{V}_j \\given q_{t}=Q_i) = B_{ij}$, $\\Prob(q_{t}=Q_j \\given q_{t-1}=Q_{i})=A_{ij}$, and $q_0$ is the predefined initial tag `TAG.vocab.stoi[TAG.init_token]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1b8b7",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Goal 1(c): Viterbi algorithm\n",
    "\n",
    "Implement the `predict` method, which should use the Viterbi algorithm to find the most likely sequence of tags for a sequence of `words`.\n",
    "\n",
    "> Warning: It may take up to 30 minutes to tag the entire test set depending on your implementation. (A fully tensorized implementation can be much faster though.) We highly recommend that you begin by experimenting with your code using a _very small subset_ of the dataset, say two or three sentences, ramping up from there.\n",
    "\n",
    "> Hint: Consider how to use vectorized computations where possible for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0af98",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We've provided you with the `evaluate` function, which takes a dataset iterator and uses `predict` on each sentence in each batch, comparing against the gold tags, to determine the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca34fbe",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class HMMTagger():\n",
    "  def __init__ (self, text, tag):\n",
    "    self.text = text\n",
    "    self.tag = tag\n",
    "    self.V = len(text.vocab.itos)    # vocabulary size\n",
    "    self.N = len(tag.vocab.itos)     # state space size\n",
    "    self.initial_state_id = tag.vocab.stoi[tag.init_token]\n",
    "    self.pad_state_id = tag.vocab.stoi[tag.pad_token]\n",
    "    self.pad_word_id = text.vocab.stoi[text.pad_token]\n",
    "  \n",
    "  def train_A(self, iterator, delta):\n",
    "    \"\"\"Returns A for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n",
    "    # Create A table\n",
    "    A = torch.zeros(self.N, self.N, device=device)\n",
    "\n",
    "    #TODO: Add your solution from Goal 1(a) here.\n",
    "    #      The returned value should be a tensor for the A matrix\n",
    "    #      of size N x N.\n",
    "    batchs = iter(iterator)\n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batchs)\n",
    "            for i in range(len(batch.tag[0])):\n",
    "                sentence_tags = batch.tag[:,i]\n",
    "                for j in range(len(sentence_tags)-1):\n",
    "                    q_prev = sentence_tags[j].item()\n",
    "                    q_t = sentence_tags[j+1].item()\n",
    "                    A[q_prev][q_t] += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    for i in range(self.N):\n",
    "        A[i] = (A[i] + delta) / (A[i].sum().item() + delta * self.V)\n",
    "    \n",
    "    return A\n",
    "\n",
    "  def train_B(self, iterator, delta):\n",
    "    \"\"\"Returns B for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n",
    "    # Create B\n",
    "    B = torch.zeros(self.N, self.V, device=device)\n",
    "    \n",
    "    #TODO: Add your solution from Goal 1 (b) here.\n",
    "    #      The returned value should be a tensor for the $B$ matrix\n",
    "    #      of size N x V.\n",
    "    \n",
    "    batchs = iter(iterator)\n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batchs)\n",
    "            for i in range(len(batch.tag[0])):\n",
    "                sentence_tags = batch.tag[:,i]\n",
    "                sentence_text = batch.text[:,i]\n",
    "                for j in range(len(sentence_tags)):\n",
    "                    q_t = sentence_tags[j].item()\n",
    "                    w_t = sentence_text[j].item()\n",
    "                    B[q_t][w_t] += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "    for i in range(self.N):\n",
    "        B[i] = (B[i]+ delta) / (B[i].sum().item() + delta * self.V)   \n",
    "    \n",
    "    return B\n",
    "\n",
    "  def train_all(self, iterator, delta=0.01):\n",
    "    \"\"\"Stores A and B (actually, their logs) for training dataset `iterator`.\"\"\"\n",
    "    self.log_A = self.train_A(iterator, delta).log()\n",
    "    self.log_B = self.train_B(iterator, delta).log()\n",
    "    \n",
    "  def predict(self, words):\n",
    "    \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\n",
    "    Arguments:\n",
    "      words: a tensor of size (seq_len,)\n",
    "    Returns:\n",
    "      a list of tag ids\n",
    "    \"\"\"\n",
    "    #TODO: Add your solution from Goal 1 (c) here.\n",
    "    #      The returned value should be a list of tag ids.\n",
    "    seq_len = words.size(0)\n",
    "    path_probs = torch.zeros(self.N, seq_len, device=device)\n",
    "    path_pointers = torch.zeros(self.N, seq_len, device=device)\n",
    "    \n",
    "    path_probs[self.initial_state_id][0] = 1\n",
    "    path_probs[:,0] = path_probs[:,0].log()\n",
    "    \n",
    "    for t in range(1, seq_len):\n",
    "        prev_probs = path_probs[:,t-1]\n",
    "        w_t = words[t].item()\n",
    "        b_t = self.log_B[:,w_t]\n",
    "        for tag in range(self.N):\n",
    "            a_tag = self.log_A[:,tag]\n",
    "            probs_for_tag = prev_probs + a_tag + b_t[tag].item()\n",
    "            path_probs[tag][t] = probs_for_tag.max().item()\n",
    "            path_pointers[tag][t] = probs_for_tag.argmax().item()\n",
    "    best_last_pointer = path_probs[:,seq_len-1].argmax().item()\n",
    "    bestpath = [best_last_pointer]\n",
    "    for t in reversed(range(1,seq_len)):\n",
    "        best_last_pointer = int(path_pointers[best_last_pointer][t].item())\n",
    "        bestpath.insert(0, best_last_pointer)\n",
    "    return bestpath\n",
    "    \n",
    "  def evaluate(self, iterator):\n",
    "    \"\"\"Returns the model's token accuracy on a given dataset `iterator`.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(iterator, leave=False):\n",
    "      for sent_id in range(len(batch)):\n",
    "        words = batch.text[:, sent_id]\n",
    "        words = words[words.ne(self.pad_word_id)] # remove paddings\n",
    "        tags_gold = batch.tag[:, sent_id]\n",
    "        tags_pred = self.predict(words)\n",
    "        for tag_gold, tag_pred in zip(tags_gold, tags_pred):\n",
    "          if tag_gold == self.pad_state_id:  # stop once we hit padding\n",
    "            break\n",
    "          else:\n",
    "            total += 1\n",
    "            if tag_pred == tag_gold:\n",
    "              correct += 1\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c68bf",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Putting everything together, you should now be able to train and evaluate the HMM. A correct implementation can be expected to reach above **90% test set accuracy** after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb44ae40",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.916\n",
      "Test accuracy:     0.907\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "hmm_tagger = HMMTagger(TEXT, TAG)\n",
    "hmm_tagger.train_all(train_iter)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {hmm_tagger.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {hmm_tagger.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1648b",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# RNN for Sequence Labeling\n",
    "\n",
    "HMMs work quite well for this sequence labeling task. Now let's take an alternative (and more trendy) approach: RNN/LSTM-based sequence labeling. Similar to the HMM part of this project, you will also need to train a model on the training data, and then use the trained model to decode and evaluate some testing data.\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/rnn-unfolded-figure.png\" width=600 align=right />\n",
    "\n",
    "After unfolding an RNN, the cell at time $t$ generates the observed output $\\vect{y}_t$ based on the input $\\vect{x}_t$ and the hidden state of the previous cell $\\vect{h}_{t-1}$, according to the following equations.\n",
    "\n",
    "\\begin{align*}\n",
    "\\vect{h}_t &=  \\sigma(\\vect{U} \\vect{x}_t + \\vect{V} \\vect{h}_{t-1}) \\\\\n",
    "\\vect{\\hat y}_t &= \\softmax(\\vect{W} \\vect{h}_t)\n",
    "\\end{align*}\n",
    "\n",
    "The parameters here are the elements of the matrices $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$. Similar to the last project segment, we will perform the forward computation, calculate the loss, and then perform the backward computation to compute the gradients with respect to these model parameters. Finally, we will adjust the parameters opposite the direction of the gradients to minimize the loss, repeating until convergence.\n",
    "\n",
    "You've seen these kinds of neural network models before, for language modeling in lab 2-3 and sequence labeling in lab 2-5. The code there should be very helpful in implementing an `RNNTagger` class below. Consequently, we've provided very little guidance on the implementation. We do recommend you follow the steps below however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c4e9e",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Goal 2(a): RNN training\n",
    "\n",
    "Implement the forward pass of the RNN tagger and the loss function. A reasonable way to proceed is to implement the following methods:\n",
    "\n",
    "1. `forward(self, text_batch)`: Performs the RNN forward computation over a whole `text_batch` (`batch.text` in the above data loading example). The `text_batch` will be of shape `max_length x batch_size`. You might run it through the following layers: an embedding layer, which maps each token index to an embedding of size `embedding_size` (so that the size of the mapped batch becomes `max_length x batch_size x embedding_size`); then an RNN, which maps each token embedding to a vector of `hidden_size` (the size of all outputs is `max_length x batch_size x hidden_size`); then a linear layer, which maps each RNN output element to a vector of size $N$ (which is commonly referred to as \"logits\", recall that $N=|Q|$, the size of the tag set).\n",
    "\n",
    "This function is expected to return `logits`, which provides a logit for each tag of each word of each sentence in the batch (structured as a tensor of size `max_length x batch_size x N`). \n",
    "\n",
    "> You might find the following functions useful: \n",
    ">\n",
    "> * [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "> * [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "> * [`nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "\n",
    "2. `compute_loss(self, logits, tags)`: Computes the loss for a batch by comparing `logits` of a batch returned by `forward` to `tags`, which stores the true tag ids for the batch. Thus `logits` is a tensor of size `max_length x batch_size x N`, and `tags` is a tensor of size `max_length x batch_size`. Note that the criterion functions in `torch` expect outputs of a certain shape, so you might need to perform some shape conversions.\n",
    "\n",
    "> You might find [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) from the last project segment useful. Note that if you use `nn.CrossEntropyLoss` then you should not use a softmax layer at the end since that's already absorbed into the loss function. Alternatively, you can use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) as the final sublayer in the forward pass, but then you need to use [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), which does not contain its own softmax. We recommend the former, since working in log space is usually more numerically stable.\n",
    "\n",
    "> Be careful about the shapes/dimensions of tensors. You might find [`torch.Tensor.view`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) useful for reshaping tensors.\n",
    "\n",
    "3. `train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001)`: Trains the model on training data generated by the iterator `train_iter` and validation data `val_iter`.The `epochs` and `learning_rate` variables are the number of epochs (number of times to run through the training data) to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine which model was the best one as the epocks go by. Notice that our code below assumes that during training the best model is stored so that `rnn_tagger.load_state_dict(rnn_tagger.best_model)` restores the parameters of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18b5b7",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Goal 2(b) RNN decoding\n",
    "\n",
    "Implement a method to predict the tag sequence associated with a sequence of words:\n",
    "\n",
    "1. `predict(self, text_batch)`: Returns the batched predicted tag sequences associated with a batch of sentences.\n",
    "2. `def evaluate(self, iterator)`: Returns the accuracy of the trained tagger on a dataset provided by `iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc758a6",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module): \n",
    "    def __init__(self, text, tag, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.tag = tag\n",
    "        self.N = len(tag.vocab.itos)\n",
    "        self.V = len(text.vocab.itos)\n",
    "        self.initial_state_id = tag.vocab.stoi[tag.init_token]\n",
    "        self.pad_state_id = tag.vocab.stoi[tag.pad_token]\n",
    "        self.pad_word_id = text.vocab.stoi[text.pad_token]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Create essential modules\n",
    "        self.word_embeddings = nn.Embedding(self.V, embedding_size) # Lookup layer\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size)\n",
    "        self.hidden2output = nn.Linear(hidden_size, self.N)\n",
    "\n",
    "        # Create loss function\n",
    "        pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n",
    "\n",
    "        # initialize parameters randomly\n",
    "        torch.manual_seed(1234)\n",
    "        for p in self.parameters():\n",
    "            p.data.uniform_(-0.2, 0.2)\n",
    "\n",
    "    def forward(self, text_batch):\n",
    "        logits = self.word_embeddings(text_batch)\n",
    "        logits = self.rnn(logits)[0]\n",
    "        logits = self.hidden2output(logits)\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, logits, tags):\n",
    "        return self.loss_function(logits.view(-1, self.N), tags.view(-1))\n",
    "    \n",
    "    def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "        self.train()\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = -float('inf')\n",
    "        best_model = None\n",
    "        \n",
    "        for epoch in range(epochs): \n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_iter):\n",
    "                self.zero_grad()\n",
    "                words = batch.text\n",
    "                tags = batch.tag\n",
    "                logits = self.forward(words)\n",
    "                loss = self.compute_loss(logits, tags)\n",
    "                \n",
    "                (loss/words.size(1)).backward()\n",
    "                optim.step()\n",
    "                \n",
    "                total += 1\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            validation_accuracy = self.evaluate(val_iter)\n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                self.best_model = copy.deepcopy(self.state_dict())\n",
    "            epoch_loss = running_loss / total\n",
    "            print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "                     f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "    \n",
    "    def predict(self, text_batch):\n",
    "        logits = self.forward(text_batch).view(-1, self.N)\n",
    "        predicts = logits.argmax(1)\n",
    "        return predicts\n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pad_id = TAG.vocab.stoi[TAG.pad_token]\n",
    "        for batch in tqdm(iterator):\n",
    "            words = batch.text\n",
    "            tags = batch.tag.view(-1)\n",
    "            tags_pred = self.predict(words)\n",
    "            \n",
    "            for i in range(tags.size(0)):\n",
    "                if tags[i]!=self.pad_state_id:\n",
    "                    total+=1\n",
    "                    if tags[i]==tags_pred[i]:\n",
    "                        correct+=1\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        return correct/total\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff497156",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now train your tagger on the training and validation set.\n",
    "Run the cell below to train an RNN, and evaluate it. A proper implementation should reach about **95%+ accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0542af2",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7343ac0de23944bdb9aebcd6e3d91d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84eceb7447084420bca4e7bb8230cfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 563.7037 Validation accuracy: 0.7541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e690a375310144eb90229b58ea0eec9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712dbd77b4334ed5ab0dd242e30a1e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 231.5850 Validation accuracy: 0.8721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e37ddfe1084674a911d0b6d5aae22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9289b614a27b46388160425548b6b270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 132.9385 Validation accuracy: 0.9283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24426a1c8d654f44b3055acb36acf291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d0fe0e0512460a9c2967c454b8caf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 88.2456 Validation accuracy: 0.9430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53ac2ed3d7d47a3966ea822131b79f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3825b85c3b420ea12e3ba3a5f34c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 66.9518 Validation accuracy: 0.9498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ce77d347949f8b99bad4b4a455464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54188be49cf545beb1ee5ccb5c1a1557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 54.1702 Validation accuracy: 0.9566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0362f5aa05cf44ab981ec6829f72c0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f811e4a05054e0daf3f32961f18b057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 45.3603 Validation accuracy: 0.9623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f1a0e473c8492e8b00bf0c8b4bc20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3036c882fe4c4c4b859f4c0dcd01ab6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 38.7343 Validation accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ffc1b5725c47a88a65f44495c69779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b192aa8dc348ccaf07099f59e07218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 33.5869 Validation accuracy: 0.9698\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a16ee4bd0904bc9a4595fe5e4fd1ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae478595a0104ca78e5e70d32f6e9471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 29.5124 Validation accuracy: 0.9728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbdcac2739c436aa0062a5f2ff49504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e516a0be99524a0ea81cc88b7d8d51bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.981\n",
      "Test accuracy:     0.970\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "rnn_tagger = RNNTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n",
    "rnn_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n",
    "rnn_tagger.load_state_dict(rnn_tagger.best_model)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {rnn_tagger.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {rnn_tagger.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9aeda7",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# LSTM for slot filling\n",
    "\n",
    "Did your RNN perform better than HMM? How much better was it? Was that expected? \n",
    "\n",
    "RNNs tend to exhibit the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). To remedy this, the Long-Short Term Memory (LSTM) model was introduced. In PyTorch, we can simply use [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n",
    "\n",
    "In this section, you'll implement an LSTM model for slot filling. If you've got the RNN model well implemented, this should be extremely straightforward. Just copy and paste your solution, change the call to `nn.RNN` to a call to `nn.LSTM`, and make any other minor adjustments that are necessary. In particular, LSTMs have _two_ recurrent parts, `h` and `c`. You'll thus need to initialize both of these when performing forward computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a15204bb",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, text, tag, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.tag = tag\n",
    "        self.N = len(tag.vocab.itos)\n",
    "        self.V = len(text.vocab.itos)\n",
    "        self.initial_state_id = tag.vocab.stoi[tag.init_token]\n",
    "        self.pad_state_id = tag.vocab.stoi[tag.pad_token]\n",
    "        self.pad_word_id = text.vocab.stoi[text.pad_token]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Create essential modules\n",
    "        self.word_embeddings = nn.Embedding(self.V, embedding_size) # Lookup layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size)\n",
    "        self.hidden2output = nn.Linear(hidden_size, self.N)\n",
    "\n",
    "        # Create loss function\n",
    "        pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n",
    "\n",
    "        # initialize parameters randomly\n",
    "        torch.manual_seed(1234)\n",
    "        for p in self.parameters():\n",
    "            p.data.uniform_(-0.2, 0.2)\n",
    "\n",
    "    def forward(self, text_batch):\n",
    "        logits = self.word_embeddings(text_batch)\n",
    "        logits = self.lstm(logits)[0]\n",
    "        logits = self.hidden2output(logits)\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, logits, tags):\n",
    "        return self.loss_function(logits.view(-1, self.N), tags.view(-1))\n",
    "    \n",
    "    def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "        self.train()\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = -float('inf')\n",
    "        best_model = None\n",
    "        \n",
    "        for epoch in range(epochs): \n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_iter):\n",
    "                self.zero_grad()\n",
    "                words = batch.text\n",
    "                tags = batch.tag\n",
    "                logits = self.forward(words)\n",
    "                loss = self.compute_loss(logits, tags)\n",
    "                \n",
    "                (loss/words.size(1)).backward()\n",
    "                optim.step()\n",
    "                \n",
    "                total += 1\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            validation_accuracy = self.evaluate(val_iter)\n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                self.best_model = copy.deepcopy(self.state_dict())\n",
    "            epoch_loss = running_loss / total\n",
    "            print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "                     f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "    \n",
    "    def predict(self, text_batch):\n",
    "        logits = self.forward(text_batch).view(-1, self.N)\n",
    "        predicts = logits.argmax(1)\n",
    "        return predicts\n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pad_id = TAG.vocab.stoi[TAG.pad_token]\n",
    "        for batch in tqdm(iterator):\n",
    "            words = batch.text\n",
    "            tags = batch.tag.view(-1)\n",
    "            tags_pred = self.predict(words)\n",
    "            \n",
    "            for i in range(tags.size(0)):\n",
    "                if tags[i]!=self.pad_state_id:\n",
    "                    total+=1\n",
    "                    if tags[i]==tags_pred[i]:\n",
    "                        correct+=1\n",
    "                        \n",
    "        return correct/total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162aef4",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Run the cell below to train an LSTM, and evaluate it. A proper implementation should reach about **95%+ accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "980676af",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5140f985acb4d49b5c8d43a2ddd00c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49e4e3b675c480484465854ed275163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 623.2899 Validation accuracy: 0.7080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65c6e892df948c086e0b8ea48b7c5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bb6f43f6a440b2948bb4cdacc800d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 266.0129 Validation accuracy: 0.8402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea2745bff274300bbb18a311fdd23e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12eb3391773a4377a5c7804f96dcabcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 187.7253 Validation accuracy: 0.8762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3d706375454854a343ee82b635a904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9989ec28a94f0f9bd9480d427139ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 141.7275 Validation accuracy: 0.9081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db12dd7f21ac49dd843e9926b7f7dd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2a7ffc07844f09bcf6a7a647fdea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 109.1600 Validation accuracy: 0.9301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e00bf78d6b400db998ffd3787b891b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44cad62e51148d6b511a9b423f7c417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 87.4566 Validation accuracy: 0.9388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f74d850e92474981125f816fd05cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bc0a6657a34bfba415b013b0b6a223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 72.7418 Validation accuracy: 0.9421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba788ff564e4410913be2174b9294b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e50392a5a94458ebf8da378ad0294c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 62.1800 Validation accuracy: 0.9495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0238eca8e0e4e88bfebf06f5138e9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf6ab015c154b02a4779ad3dea348e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 54.1019 Validation accuracy: 0.9527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb2c9132b3d4ed58fd02f6dcc747f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ad1311bd034bd390b2e2ff2893336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 47.7549 Validation accuracy: 0.9572\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13d911b6f31406cab5032192ec1ab4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d55f085a9e443c48291be663e311777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.965\n",
      "Test accuracy:     0.958\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "lstm_tagger = LSTMTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n",
    "lstm_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n",
    "lstm_tagger.load_state_dict(lstm_tagger.best_model)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {lstm_tagger.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {lstm_tagger.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d4111",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Goal 4: Compare HMM to RNN/LSTM with different amounts of training data\n",
    "\n",
    "Vary the amount of training data and compare the performance of HMM to RNN or LSTM (Since RNN is similar to LSTM, picking one of them is enough.) Discuss the pros and cons of HMM and RNN/LSTM based on your experiments.\n",
    "\n",
    "> This part is more open-ended. We're looking for thoughtful experiments and analysis of the results, not any particular result or conclusion.\n",
    "\n",
    "The code below shows how to subsample the training set with downsample ratio `ratio`. To speedup evaluation we only use 50 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a037bc02",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "test_size = 50\n",
    "\n",
    "# Set random seeds to make sure subsampling is the same for HMM and RNN\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n",
    "            fields=fields, \n",
    "            path='./data/', \n",
    "            train='atis.train.txt', \n",
    "            validation='atis.dev.txt',\n",
    "            test='atis.test.txt')\n",
    "\n",
    "# Subsample\n",
    "random.shuffle(train.examples)\n",
    "train.examples = train.examples[:int(math.floor(len(train.examples)*ratio))]\n",
    "random.shuffle(test.examples)\n",
    "test.examples = test.examples[:test_size]\n",
    "\n",
    "# Rebuild vocabulary\n",
    "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n",
    "TAG.build_vocab(train.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4010fb98",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM with BATCH_SIZE=20; ratio=0.1; test_size=40\n",
      "Training accuracy: 0.912\n",
      "Test accuracy:     0.857\n",
      "Epoch: 0 Loss: 1128.8748 Validation accuracy: 0.6392\n",
      "Epoch: 1 Loss: 741.1990 Validation accuracy: 0.6392\n",
      "Epoch: 2 Loss: 534.3540 Validation accuracy: 0.6392\n",
      "Epoch: 3 Loss: 486.2779 Validation accuracy: 0.7080\n",
      "Epoch: 4 Loss: 456.7896 Validation accuracy: 0.7080\n",
      "Epoch: 5 Loss: 423.0286 Validation accuracy: 0.7080\n",
      "Epoch: 6 Loss: 379.7874 Validation accuracy: 0.7347\n",
      "Epoch: 7 Loss: 333.3264 Validation accuracy: 0.7585\n",
      "Epoch: 8 Loss: 298.8895 Validation accuracy: 0.7891\n",
      "Epoch: 9 Loss: 273.7859 Validation accuracy: 0.8151\n",
      "RNN with BATCH_SIZE=20; ratio=0.1; test_size=40\n",
      "Training accuracy: 0.811\n",
      "Test accuracy:     0.830\n",
      "HMM with BATCH_SIZE=20; ratio=0.1; test_size=80\n",
      "Training accuracy: 0.920\n",
      "Test accuracy:     0.883\n",
      "Epoch: 0 Loss: 1182.3215 Validation accuracy: 0.5242\n",
      "Epoch: 1 Loss: 813.2898 Validation accuracy: 0.6392\n",
      "Epoch: 2 Loss: 559.7973 Validation accuracy: 0.6392\n",
      "Epoch: 3 Loss: 493.2642 Validation accuracy: 0.7080\n",
      "Epoch: 4 Loss: 463.8060 Validation accuracy: 0.7080\n",
      "Epoch: 5 Loss: 429.6064 Validation accuracy: 0.7080\n",
      "Epoch: 6 Loss: 389.1020 Validation accuracy: 0.7080\n",
      "Epoch: 7 Loss: 344.6482 Validation accuracy: 0.7586\n",
      "Epoch: 8 Loss: 306.0016 Validation accuracy: 0.8011\n",
      "Epoch: 9 Loss: 274.9712 Validation accuracy: 0.8146\n",
      "RNN with BATCH_SIZE=20; ratio=0.1; test_size=80\n",
      "Training accuracy: 0.815\n",
      "Test accuracy:     0.803\n",
      "HMM with BATCH_SIZE=20; ratio=0.1; test_size=200\n",
      "Training accuracy: 0.905\n",
      "Test accuracy:     0.882\n",
      "Epoch: 0 Loss: 1119.9609 Validation accuracy: 0.5729\n",
      "Epoch: 1 Loss: 777.0579 Validation accuracy: 0.6392\n",
      "Epoch: 2 Loss: 517.3261 Validation accuracy: 0.7080\n",
      "Epoch: 3 Loss: 459.5614 Validation accuracy: 0.7080\n",
      "Epoch: 4 Loss: 434.4949 Validation accuracy: 0.7080\n",
      "Epoch: 5 Loss: 411.0320 Validation accuracy: 0.7080\n",
      "Epoch: 6 Loss: 385.4109 Validation accuracy: 0.7080\n",
      "Epoch: 7 Loss: 351.6840 Validation accuracy: 0.7080\n",
      "Epoch: 8 Loss: 311.5224 Validation accuracy: 0.7592\n",
      "Epoch: 9 Loss: 277.0468 Validation accuracy: 0.8060\n",
      "RNN with BATCH_SIZE=20; ratio=0.1; test_size=200\n",
      "Training accuracy: 0.810\n",
      "Test accuracy:     0.804\n",
      "HMM with BATCH_SIZE=20; ratio=0.1; test_size=500\n",
      "Training accuracy: 0.914\n",
      "Test accuracy:     0.882\n",
      "Epoch: 0 Loss: 1168.2067 Validation accuracy: 0.6392\n",
      "Epoch: 1 Loss: 829.1846 Validation accuracy: 0.6392\n",
      "Epoch: 2 Loss: 559.4784 Validation accuracy: 0.6392\n",
      "Epoch: 3 Loss: 492.2838 Validation accuracy: 0.7080\n",
      "Epoch: 4 Loss: 464.0685 Validation accuracy: 0.7080\n",
      "Epoch: 5 Loss: 439.9941 Validation accuracy: 0.7080\n",
      "Epoch: 6 Loss: 419.0415 Validation accuracy: 0.7080\n",
      "Epoch: 7 Loss: 395.1312 Validation accuracy: 0.7080\n",
      "Epoch: 8 Loss: 364.4520 Validation accuracy: 0.7105\n",
      "Epoch: 9 Loss: 327.4932 Validation accuracy: 0.7613\n",
      "RNN with BATCH_SIZE=20; ratio=0.1; test_size=500\n",
      "Training accuracy: 0.761\n",
      "Test accuracy:     0.755\n",
      "HMM with BATCH_SIZE=20; ratio=0.3; test_size=40\n",
      "Training accuracy: 0.913\n",
      "Test accuracy:     0.916\n",
      "Epoch: 0 Loss: 888.1861 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 461.6044 Validation accuracy: 0.7080\n",
      "Epoch: 2 Loss: 364.7464 Validation accuracy: 0.7632\n",
      "Epoch: 3 Loss: 274.2156 Validation accuracy: 0.8223\n",
      "Epoch: 4 Loss: 218.1587 Validation accuracy: 0.8492\n",
      "Epoch: 5 Loss: 183.0774 Validation accuracy: 0.8780\n",
      "Epoch: 6 Loss: 156.6430 Validation accuracy: 0.8969\n",
      "Epoch: 7 Loss: 135.6992 Validation accuracy: 0.9082\n",
      "Epoch: 8 Loss: 118.6820 Validation accuracy: 0.9253\n",
      "Epoch: 9 Loss: 104.5193 Validation accuracy: 0.9326\n",
      "RNN with BATCH_SIZE=20; ratio=0.3; test_size=40\n",
      "Training accuracy: 0.937\n",
      "Test accuracy:     0.942\n",
      "HMM with BATCH_SIZE=20; ratio=0.3; test_size=80\n",
      "Training accuracy: 0.913\n",
      "Test accuracy:     0.910\n",
      "Epoch: 0 Loss: 835.2517 Validation accuracy: 0.6392\n",
      "Epoch: 1 Loss: 471.8930 Validation accuracy: 0.7080\n",
      "Epoch: 2 Loss: 374.3144 Validation accuracy: 0.7485\n",
      "Epoch: 3 Loss: 279.9462 Validation accuracy: 0.8103\n",
      "Epoch: 4 Loss: 235.2413 Validation accuracy: 0.8405\n",
      "Epoch: 5 Loss: 201.4327 Validation accuracy: 0.8615\n",
      "Epoch: 6 Loss: 172.9968 Validation accuracy: 0.8774\n",
      "Epoch: 7 Loss: 149.8857 Validation accuracy: 0.8905\n",
      "Epoch: 8 Loss: 130.1492 Validation accuracy: 0.9189\n",
      "Epoch: 9 Loss: 114.0299 Validation accuracy: 0.9293\n",
      "RNN with BATCH_SIZE=20; ratio=0.3; test_size=80\n",
      "Training accuracy: 0.931\n",
      "Test accuracy:     0.929\n",
      "HMM with BATCH_SIZE=20; ratio=0.3; test_size=200\n",
      "Training accuracy: 0.915\n",
      "Test accuracy:     0.904\n",
      "Epoch: 0 Loss: 818.8136 Validation accuracy: 0.6392\n",
      "Epoch: 1 Loss: 452.3134 Validation accuracy: 0.7080\n",
      "Epoch: 2 Loss: 336.9323 Validation accuracy: 0.8030\n",
      "Epoch: 3 Loss: 257.8126 Validation accuracy: 0.8313\n",
      "Epoch: 4 Loss: 215.6841 Validation accuracy: 0.8402\n",
      "Epoch: 5 Loss: 180.7543 Validation accuracy: 0.8744\n",
      "Epoch: 6 Loss: 151.6772 Validation accuracy: 0.9031\n",
      "Epoch: 7 Loss: 128.9294 Validation accuracy: 0.9157\n",
      "Epoch: 8 Loss: 111.5458 Validation accuracy: 0.9196\n",
      "Epoch: 9 Loss: 98.1087 Validation accuracy: 0.9267\n",
      "RNN with BATCH_SIZE=20; ratio=0.3; test_size=200\n",
      "Training accuracy: 0.934\n",
      "Test accuracy:     0.919\n",
      "HMM with BATCH_SIZE=20; ratio=0.3; test_size=500\n",
      "Training accuracy: 0.916\n",
      "Test accuracy:     0.889\n",
      "Epoch: 0 Loss: 862.3377 Validation accuracy: 0.6392\n",
      "Epoch: 1 Loss: 456.5286 Validation accuracy: 0.7080\n",
      "Epoch: 2 Loss: 356.6419 Validation accuracy: 0.7637\n",
      "Epoch: 3 Loss: 265.0396 Validation accuracy: 0.8323\n",
      "Epoch: 4 Loss: 209.5375 Validation accuracy: 0.8500\n",
      "Epoch: 5 Loss: 176.7355 Validation accuracy: 0.8702\n",
      "Epoch: 6 Loss: 153.2418 Validation accuracy: 0.8894\n",
      "Epoch: 7 Loss: 133.7454 Validation accuracy: 0.9142\n",
      "Epoch: 8 Loss: 117.1827 Validation accuracy: 0.9196\n",
      "Epoch: 9 Loss: 103.4889 Validation accuracy: 0.9259\n",
      "RNN with BATCH_SIZE=20; ratio=0.3; test_size=500\n",
      "Training accuracy: 0.937\n",
      "Test accuracy:     0.923\n",
      "HMM with BATCH_SIZE=20; ratio=0.5; test_size=40\n",
      "Training accuracy: 0.917\n",
      "Test accuracy:     0.903\n",
      "Epoch: 0 Loss: 751.4835 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 386.1452 Validation accuracy: 0.8038\n",
      "Epoch: 2 Loss: 252.8003 Validation accuracy: 0.8346\n",
      "Epoch: 3 Loss: 190.2982 Validation accuracy: 0.8857\n",
      "Epoch: 4 Loss: 143.6523 Validation accuracy: 0.9164\n",
      "Epoch: 5 Loss: 113.1662 Validation accuracy: 0.9261\n",
      "Epoch: 6 Loss: 93.3433 Validation accuracy: 0.9337\n",
      "Epoch: 7 Loss: 79.7567 Validation accuracy: 0.9398\n",
      "Epoch: 8 Loss: 69.9193 Validation accuracy: 0.9441\n",
      "Epoch: 9 Loss: 62.1447 Validation accuracy: 0.9489\n",
      "RNN with BATCH_SIZE=20; ratio=0.5; test_size=40\n",
      "Training accuracy: 0.955\n",
      "Test accuracy:     0.931\n",
      "HMM with BATCH_SIZE=20; ratio=0.5; test_size=80\n",
      "Training accuracy: 0.914\n",
      "Test accuracy:     0.893\n",
      "Epoch: 0 Loss: 725.9815 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 400.9359 Validation accuracy: 0.7132\n",
      "Epoch: 2 Loss: 262.5803 Validation accuracy: 0.8364\n",
      "Epoch: 3 Loss: 192.9405 Validation accuracy: 0.8746\n",
      "Epoch: 4 Loss: 148.2961 Validation accuracy: 0.9123\n",
      "Epoch: 5 Loss: 117.4247 Validation accuracy: 0.9308\n",
      "Epoch: 6 Loss: 94.6646 Validation accuracy: 0.9342\n",
      "Epoch: 7 Loss: 79.6025 Validation accuracy: 0.9390\n",
      "Epoch: 8 Loss: 69.2457 Validation accuracy: 0.9436\n",
      "Epoch: 9 Loss: 61.5254 Validation accuracy: 0.9486\n",
      "RNN with BATCH_SIZE=20; ratio=0.5; test_size=80\n",
      "Training accuracy: 0.955\n",
      "Test accuracy:     0.932\n",
      "HMM with BATCH_SIZE=20; ratio=0.5; test_size=200\n",
      "Training accuracy: 0.915\n",
      "Test accuracy:     0.898\n",
      "Epoch: 0 Loss: 764.5997 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 371.2308 Validation accuracy: 0.7656\n",
      "Epoch: 2 Loss: 244.8444 Validation accuracy: 0.8313\n",
      "Epoch: 3 Loss: 188.6355 Validation accuracy: 0.8855\n",
      "Epoch: 4 Loss: 142.7622 Validation accuracy: 0.9141\n",
      "Epoch: 5 Loss: 113.6288 Validation accuracy: 0.9271\n",
      "Epoch: 6 Loss: 93.8153 Validation accuracy: 0.9357\n",
      "Epoch: 7 Loss: 79.5500 Validation accuracy: 0.9385\n",
      "Epoch: 8 Loss: 68.9869 Validation accuracy: 0.9427\n",
      "Epoch: 9 Loss: 61.1614 Validation accuracy: 0.9461\n",
      "RNN with BATCH_SIZE=20; ratio=0.5; test_size=200\n",
      "Training accuracy: 0.955\n",
      "Test accuracy:     0.946\n",
      "HMM with BATCH_SIZE=20; ratio=0.5; test_size=500\n",
      "Training accuracy: 0.914\n",
      "Test accuracy:     0.897\n",
      "Epoch: 0 Loss: 717.9303 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 368.1751 Validation accuracy: 0.7657\n",
      "Epoch: 2 Loss: 248.2462 Validation accuracy: 0.8374\n",
      "Epoch: 3 Loss: 189.7852 Validation accuracy: 0.8726\n",
      "Epoch: 4 Loss: 143.2127 Validation accuracy: 0.9159\n",
      "Epoch: 5 Loss: 112.5131 Validation accuracy: 0.9297\n",
      "Epoch: 6 Loss: 92.6496 Validation accuracy: 0.9330\n",
      "Epoch: 7 Loss: 79.0413 Validation accuracy: 0.9395\n",
      "Epoch: 8 Loss: 69.1193 Validation accuracy: 0.9433\n",
      "Epoch: 9 Loss: 61.4485 Validation accuracy: 0.9497\n",
      "RNN with BATCH_SIZE=20; ratio=0.5; test_size=500\n",
      "Training accuracy: 0.956\n",
      "Test accuracy:     0.947\n",
      "HMM with BATCH_SIZE=20; ratio=0.7; test_size=40\n",
      "Training accuracy: 0.913\n",
      "Test accuracy:     0.906\n",
      "Epoch: 0 Loss: 609.2667 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 296.9291 Validation accuracy: 0.8261\n",
      "Epoch: 2 Loss: 193.5704 Validation accuracy: 0.8844\n",
      "Epoch: 3 Loss: 134.9334 Validation accuracy: 0.9207\n",
      "Epoch: 4 Loss: 100.3512 Validation accuracy: 0.9336\n",
      "Epoch: 5 Loss: 79.2951 Validation accuracy: 0.9396\n",
      "Epoch: 6 Loss: 66.1079 Validation accuracy: 0.9454\n",
      "Epoch: 7 Loss: 56.7160 Validation accuracy: 0.9501\n",
      "Epoch: 8 Loss: 49.3875 Validation accuracy: 0.9551\n",
      "Epoch: 9 Loss: 43.6684 Validation accuracy: 0.9585\n",
      "RNN with BATCH_SIZE=20; ratio=0.7; test_size=40\n",
      "Training accuracy: 0.966\n",
      "Test accuracy:     0.958\n",
      "HMM with BATCH_SIZE=20; ratio=0.7; test_size=80\n",
      "Training accuracy: 0.914\n",
      "Test accuracy:     0.911\n",
      "Epoch: 0 Loss: 625.4286 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 278.6634 Validation accuracy: 0.8371\n",
      "Epoch: 2 Loss: 176.4800 Validation accuracy: 0.9031\n",
      "Epoch: 3 Loss: 121.0343 Validation accuracy: 0.9270\n",
      "Epoch: 4 Loss: 91.7554 Validation accuracy: 0.9355\n",
      "Epoch: 5 Loss: 74.3619 Validation accuracy: 0.9406\n",
      "Epoch: 6 Loss: 62.6684 Validation accuracy: 0.9490\n",
      "Epoch: 7 Loss: 53.7471 Validation accuracy: 0.9543\n",
      "Epoch: 8 Loss: 46.4293 Validation accuracy: 0.9582\n",
      "Epoch: 9 Loss: 40.6285 Validation accuracy: 0.9615\n",
      "RNN with BATCH_SIZE=20; ratio=0.7; test_size=80\n",
      "Training accuracy: 0.971\n",
      "Test accuracy:     0.976\n",
      "HMM with BATCH_SIZE=20; ratio=0.7; test_size=200\n",
      "Training accuracy: 0.914\n",
      "Test accuracy:     0.900\n",
      "Epoch: 0 Loss: 628.0239 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 292.8200 Validation accuracy: 0.8246\n",
      "Epoch: 2 Loss: 192.3118 Validation accuracy: 0.8870\n",
      "Epoch: 3 Loss: 132.8370 Validation accuracy: 0.9282\n",
      "Epoch: 4 Loss: 98.6912 Validation accuracy: 0.9338\n",
      "Epoch: 5 Loss: 79.1471 Validation accuracy: 0.9421\n",
      "Epoch: 6 Loss: 66.5269 Validation accuracy: 0.9473\n",
      "Epoch: 7 Loss: 57.2625 Validation accuracy: 0.9509\n",
      "Epoch: 8 Loss: 50.1919 Validation accuracy: 0.9575\n",
      "Epoch: 9 Loss: 44.4749 Validation accuracy: 0.9584\n",
      "RNN with BATCH_SIZE=20; ratio=0.7; test_size=200\n",
      "Training accuracy: 0.968\n",
      "Test accuracy:     0.958\n",
      "HMM with BATCH_SIZE=20; ratio=0.7; test_size=500\n",
      "Training accuracy: 0.915\n",
      "Test accuracy:     0.904\n",
      "Epoch: 0 Loss: 618.3158 Validation accuracy: 0.7080\n",
      "Epoch: 1 Loss: 322.5076 Validation accuracy: 0.8331\n",
      "Epoch: 2 Loss: 194.7998 Validation accuracy: 0.8957\n",
      "Epoch: 3 Loss: 132.0925 Validation accuracy: 0.9243\n",
      "Epoch: 4 Loss: 96.1115 Validation accuracy: 0.9365\n",
      "Epoch: 5 Loss: 75.9371 Validation accuracy: 0.9420\n",
      "Epoch: 6 Loss: 63.3239 Validation accuracy: 0.9491\n",
      "Epoch: 7 Loss: 54.3939 Validation accuracy: 0.9549\n",
      "Epoch: 8 Loss: 47.5013 Validation accuracy: 0.9580\n",
      "Epoch: 9 Loss: 42.0076 Validation accuracy: 0.9632\n",
      "RNN with BATCH_SIZE=20; ratio=0.7; test_size=500\n",
      "Training accuracy: 0.971\n",
      "Test accuracy:     0.961\n",
      "FINISHED RUNNING\n"
     ]
    }
   ],
   "source": [
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n",
    "TAG.build_vocab(train.tag)\n",
    "\n",
    "ratios = [0.1, 0.3, 0.5, 0.7]\n",
    "test_sizes = [40, 80, 200, 500]\n",
    "\n",
    "res_hmm = list(list())\n",
    "res_rnn = list(list())\n",
    "\n",
    "for a_idx, ratio in enumerate(ratios):\n",
    "    \n",
    "    res_hmm.append([])\n",
    "    res_rnn.append([])\n",
    "    \n",
    "    for b_idx, test_size in enumerate(test_sizes):\n",
    "        train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n",
    "        fields=fields, \n",
    "        path='./data/', \n",
    "        train='atis.train.txt',\n",
    "        validation='atis.dev.txt',\n",
    "        test='atis.test.txt'\n",
    "            )\n",
    "\n",
    "        # Subsample\n",
    "        random.shuffle(train.examples)\n",
    "        train.examples = train.examples[:int(math.floor(len(train.examples)*ratio))]\n",
    "        random.shuffle(test.examples)\n",
    "        test.examples = test.examples[:test_size]\n",
    "\n",
    "        train_iter, test_iter = tt.data.BucketIterator.splits(\n",
    "        (train, test), \n",
    "        batch_size=BATCH_SIZE, \n",
    "        repeat=False, \n",
    "        device=device)\n",
    "\n",
    "        # Rebuild vocabulary\n",
    "        TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n",
    "        TAG.build_vocab(train.tag)\n",
    "\n",
    "        # HMM\n",
    "        # Instantiate and train classifier\n",
    "        hmm_tagger = HMMTagger(TEXT, TAG)\n",
    "        hmm_tagger.train_all(train_iter)\n",
    "        \n",
    "        hmm_train = hmm_tagger.evaluate(train_iter)\n",
    "        hmm_test = hmm_tagger.evaluate(test_iter)\n",
    "        res_hmm[a_idx].append((hmm_train, hmm_test))\n",
    "        # Evaluate model performance\n",
    "        print(f'HMM with {BATCH_SIZE=}; {ratio=}; {test_size=}')\n",
    "        print(f'Training accuracy: {res_hmm[a_idx][b_idx][0]:.3f}\\n'\n",
    "              f'Test accuracy:     {res_hmm[a_idx][b_idx][1]:.3f}')\n",
    "\n",
    "\n",
    "        # RNN\n",
    "        # Instantiate and train classifier\n",
    "        rnn_tagger = RNNTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n",
    "        rnn_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n",
    "        rnn_tagger.load_state_dict(rnn_tagger.best_model)\n",
    "        \n",
    "        rnn_train = rnn_tagger.evaluate(train_iter)\n",
    "        rnn_test = rnn_tagger.evaluate(test_iter)\n",
    "        res_rnn[a_idx].append((rnn_train, rnn_test))\n",
    "        # Evaluate model performance\n",
    "        print(f'RNN with {BATCH_SIZE=}; {ratio=}; {test_size=}')\n",
    "        print(f'Training accuracy: {res_rnn[a_idx][b_idx][0]:.3f}\\n'\n",
    "              f'Test accuracy:     {res_rnn[a_idx][b_idx][1]:.3f}')\n",
    "print('FINISHED RUNNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c73b99-3dda-4d0b-94c2-83d6aa05cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are (HMM / RNN) test accuracies for ratio and data size\n",
      "Test Size / Ratio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.857 / 0.830</td>\n",
       "      <td>0.916 / 0.942</td>\n",
       "      <td>0.903 / 0.931</td>\n",
       "      <td>0.906 / 0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.883 / 0.803</td>\n",
       "      <td>0.910 / 0.929</td>\n",
       "      <td>0.893 / 0.932</td>\n",
       "      <td>0.911 / 0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.882 / 0.804</td>\n",
       "      <td>0.904 / 0.919</td>\n",
       "      <td>0.898 / 0.946</td>\n",
       "      <td>0.900 / 0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.882 / 0.755</td>\n",
       "      <td>0.889 / 0.923</td>\n",
       "      <td>0.897 / 0.947</td>\n",
       "      <td>0.904 / 0.961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.1            0.3            0.5            0.7\n",
       "40   0.857 / 0.830  0.916 / 0.942  0.903 / 0.931  0.906 / 0.958\n",
       "80   0.883 / 0.803  0.910 / 0.929  0.893 / 0.932  0.911 / 0.976\n",
       "200  0.882 / 0.804  0.904 / 0.919  0.898 / 0.946  0.900 / 0.958\n",
       "500  0.882 / 0.755  0.889 / 0.923  0.897 / 0.947  0.904 / 0.961"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(index=test_sizes, columns=ratios)\n",
    "\n",
    "for a_idx, ratio in enumerate(ratios):\n",
    "    for b_idx, test_size in enumerate(test_sizes):\n",
    "        df.loc[test_size].iloc[a_idx] = f'{res_hmm[a_idx][b_idx][1]:.3f} / {res_rnn[a_idx][b_idx][1]:.3f}'\n",
    "\n",
    "print('Values are (HMM / RNN) test accuracies for ratio and data size')\n",
    "print('Test Size / Ratio')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ba9e0",
   "metadata": {},
   "source": [
    "_As we can see, we got some interesting findings:_\n",
    "\n",
    "_With changing the test/train ratio of the dataset, we got better results for each data size with HMM when the ratio was 0.1 - meaning small test size compared to the test corpus._\n",
    "\n",
    "_With every other test-train ratio, we get that the RNN did much better, gradually increasing test accuracy as the test part grew larger._\n",
    "_From this, we can infer that RNNs are better at generalizing the solution than HMM - especially with less data and bigger test corpus._\n",
    "\n",
    "_As we can't see any major difference by changing the data size, we can't say that changing the overall size impacts performance._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafdcf0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n",
    "\n",
    "* Was the project segment clear or unclear? Which portions?\n",
    "* Were the readings appropriate background for the project segment? \n",
    "* Are there additions or changes you think would make the project segment better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a63a9",
   "metadata": {},
   "source": [
    "_Great project - we learned a lot by implementing the different model types._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbcb70",
   "metadata": {},
   "source": [
    "hf<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Instructions for submission of the project segment\n",
    "\n",
    "This project segment should be submitted to Gradescope at <https://rebrand.ly/project2-submit-code> and <https://rebrand.ly/project2-submit-pdf>, which will be made available some time before the due date.\n",
    "\n",
    "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <https://rebrand.ly/project2-submit-code>.\n",
    "\n",
    "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <https://rebrand.ly/project2-submit-pdf>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "project2_sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "title": "CS236299 Project Segment 2: Sequence labeling â€“ The slot filling task"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
